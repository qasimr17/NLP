{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Data \n",
    "\n",
    "We will make use of the SMS Spam Collection Dataset provided by the UCI Machine Learning repository. According to the UCI site description, the SMS Spam Collection is a public set of SMS labeled messages that have been collected for mobile phone spam research. The data can be directly downloaded from [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table('SMSSpamCollection', delimiter = \"\\t\",header=None, names=['label', 'sms_message'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Binary Labels\n",
    "\n",
    "To make our ML model understand the targets, we need to convert them into 0/1 for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "# unique labels\n",
    "print(data.iloc[:, 0].unique())\n",
    "\n",
    "# function to convert string labels to binary\n",
    "target_conversion = lambda x : 0 if x == 'ham' else 1\n",
    "data['label'] = data.iloc[:, 0].apply(target_conversion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training instances\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words \n",
    "\n",
    "Similar to our conversion of the target variable, we need to feed our model numeric inputs as features as well. \n",
    "Currently, our input feature is just the sms message itself. To convert this into numeric format, we will implement the Bag of Words (BoW) technique.\n",
    "\n",
    "BoW essentially just counts the number of unique words in the entire vocabulary |V| and for each document, or sms in our case, creates a vector of counts of each of those vocabulary words.\n",
    "\n",
    "We will first implement this algorithm from scratch, before making use of scikit-learn's Count Vectorizer method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = ['Hello, how are you!', 'Win money, win from home.', 'Call me now.', 'Hello, Call hello you tomorrow?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversion to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we convert each word in the documents to lower case \n",
    "lower_case_documents = [word.lower() for word in sample_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello how are you',\n",
       " 'win money win from home',\n",
       " 'call me now',\n",
       " 'hello call hello you tomorrow']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all punctuation\n",
    "import string \n",
    "sans_punctuation_documents = [t.translate(str.maketrans('', '', string.punctuation)) for t in lower_case_documents]\n",
    "sans_punctuation_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'how', 'are', 'you'],\n",
       " ['win', 'money', 'win', 'from', 'home'],\n",
       " ['call', 'me', 'now'],\n",
       " ['hello', 'call', 'hello', 'you', 'tomorrow']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer - (word tokenizer to be specific), to split the entire corpus into individual words (based on a specific delimiter)\n",
    "# In our custom implementation, we will consider words as strings separated by whitespace \n",
    "preprocessed_documents = [text.split(' ') for text in sans_punctuation_documents]\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Frequencies\n",
    "\n",
    "Now that we have our data in the clean format we wanted, we can now create a Counter dictionary to count the frequency of unique words in each of the documents \n",
    "Here, we will create a Counter dictionary for each of the documents and store them in a `frequency_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n",
      " Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n",
      " Counter({'call': 1, 'me': 1, 'now': 1}),\n",
      " Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "from pprint import pprint\n",
    "frequency_list = [Counter(text) for text in preprocessed_documents]\n",
    "pprint(frequency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Vectorizer\n",
    "Having understood the basics of the BoW technique, we now make use of scikit-learn's own `Count Vectorizer` method, that will allow us to complete all of the above steps in a more concise manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are' 'call' 'from' 'hello' 'home' 'how' 'me' 'money' 'now' 'tomorrow'\n",
      " 'win' 'you']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# First, let's look at how this built-in method performs on the sample documents above \n",
    "X = count_vector.fit_transform(sample_documents)\n",
    "print(count_vector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that CountVectorizer takes care of the preprocessing/data cleaning steps that we had to implement ourselves for us. It makes use of certain parameter values to do this, including:\n",
    "\n",
    "* `lowercase = True`\n",
    "Default value True converts all text to lower case \n",
    "\n",
    "* `token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b`\n",
    "The default regular expression ignores all punctuation marks, treating them as delimiters, and accepting strings of length >= 2 as individual tokens/words.\n",
    "\n",
    "* `stop_words`\n",
    "By default, this is set to None. However, we can set this to 'english' to use a built-in stop word list for English, to remove them from our corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello' 'home' 'money' 'tomorrow' 'win']\n"
     ]
    }
   ],
   "source": [
    "# If we removed stop words as well\n",
    "stop_words_vectorizer = CountVectorizer(stop_words='english')\n",
    "stop_words_vectorizer.fit_transform(sample_documents)\n",
    "print(stop_words_vectorizer.get_feature_names_out())\n",
    "# Note that this is a much shorter list since the common (stop) words have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the implementation\n",
    "# Based on the feature names, CountVectorizer converts our documents into arrays of number, with index i, j corresponding \n",
    "# to the count of word i in document j\n",
    "# Word i may be found in the list generated from the get_feature_names_out() method as used above\n",
    "doc_array = X.toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can convert this matrix into a data frame for interpretation\n",
    "frequency_matrix = pd.DataFrame(doc_array, columns=count_vector.get_feature_names_out())\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential issue of using this technique is that when we have a large vocabulary size |V|, our feature_matrix\n",
    "will be quite massive and sparse since most words will not exist in every document.\n",
    "\n",
    "To mitigate this, we may make use of the stop words parameter, and also look at a technique called `tfidf` (to be implemented in another notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test sets \n",
    "\n",
    "Now, we look at implementing this BoW model on our actual dataset. But first, we will have to split our data \n",
    "into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 5572\n",
      "Rows in training set: 4457\n",
      "Rows in test set: 1115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data into 80-20 Training and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['sms_message'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total number of rows: {data.shape[0]}\")\n",
    "print(f\"Rows in training set: {X_train.shape[0]}\")\n",
    "print(f\"Rows in test set: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the BoW processing to this dataset.\n",
    "\n",
    "Important to note here that when we fit our CountVectorizer to the training dataset, it will create a feature matrix and learn the parameters based on the vocabulary in the training data. This means that if new words (not found in the training data) come up in the test data, they will not be considered by our model.\n",
    "\n",
    "The following example shows this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 1 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 2 0 0 0 0 0 1 0 1]]\n",
      "['are' 'call' 'from' 'hello' 'home' 'how' 'me' 'money' 'now' 'tomorrow'\n",
      " 'win' 'you']\n"
     ]
    }
   ],
   "source": [
    "print(count_vector.fit_transform(sample_documents).toarray())\n",
    "print(count_vector.get_feature_names_out())\n",
    "\n",
    "# Counts of 12 features/words are learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Only the 12 features/words learned above are used here\n",
    "# Even though the test data contained 3 new words, they were simply ignored\n",
    "test_doc = ['cricket football are great']\n",
    "y = count_vector.transform(test_doc)\n",
    "print(y.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the count vector\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# fit and transform the training data\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# only transform the test data (not fit)\n",
    "test_data = count_vector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes from scratch\n",
    "\n",
    "Naive Bayes is a probabilistic classifier such that for a document `d`, out of all classes $c \\in C$, it returns the class which has the maximum posterior probability given the document. This is denoted as:\n",
    "\n",
    "$\\hat{c} = \\underset{c \\in C}{\\argmax}  P(c|d)$\n",
    "\n",
    "To determine this class, the model makes use of the Bayes' rule such that:\n",
    "\n",
    "$\\hat{c} = \\underset{c \\in C}{\\argmax}  P(c|d) = \\underset{c \\in C}{\\argmax} \\frac{P(d|c)*P(c)}{P(d)}$\n",
    "\n",
    "However, since the denominator stays the same for each class, we can remove it and choose the class that maximises the simpler formula:\n",
    "\n",
    "$\\underset{c \\in C}{\\argmax} P(d|c)*P(c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability $P(c)$ is simply determined by the fraction of instances of class $c$ compared to all classes in $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likeilhood $P(d|c)$ is computed using a few simplifications.\n",
    "First, we note that each documented may be represented simply through its features (or words in our case).\n",
    "\n",
    "$P(d|c) = P(w_{1}, w_{2},...,w_{n}|c)$\n",
    "\n",
    "Next, we make use of the (naive) conditional independence assumption that the probabilities $P(w_{i}|c)$ are independent given class $c$ and hence can be 'naively' multiplied as:\n",
    "\n",
    "$P(w_{1}|c) * P(w_{2}|c) * ... * P(w_{n}|c)$\n",
    "\n",
    "This results in our final equation (with i = position of each word i in the document):\n",
    "\n",
    "$c_{NB} = \\underset{c \\in C}{\\argmax} P(c) * \\prod_{i \\in positions} P(w_{i}|c)$\n",
    "\n",
    "OR, more commonly used as:\n",
    "\n",
    "$c_{NB} = \\underset{c \\in C}{\\argmax} log(P(c)) + \\sum_{i \\in positions} log(P(w_{i}|c))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the prior probability\n",
    "\n",
    "As mentioned above, the prior probability $P(c)$ is simply determined by the fraction of training instances with class $c$ as the target label, out of all labels.\n",
    "\n",
    "Hence, $\\hat{P}(c) = \\frac{N_{c}}{N_{doc}}$ where $N_{c}$ is the number of documents of class $c$ in our training data and $N_{doc}$ is the total number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the likelihood\n",
    "\n",
    "We compute $P(w_{i}|c)$ as the fraction of times the word $w_{i}$ appears among all words in all documents of topic $c$. \n",
    "\n",
    "$\\hat{P}(w_{i}|c) = \\frac{w_{i}, c}{\\sum_{w \\in V}count(w, c)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Potential Issues\n",
    "\n",
    "There are two potential problems we may run into with our approach.\n",
    "\n",
    "1. A particular word that is in the training data, does not appear in a particular class. Meaning $\\hat{P}(w_{i}|c)$ for that particular word and class would be 0, and it's logarithm will cause an error. To resolve this, we make use of add-one smoothing so that the count for such pairs is 1 instead of 0.\n",
    "\n",
    "2. There may be words in the test data that were not encountered in the training set. To resolve this, we just ignore these new words and make sure to use only those that were found in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "We make use of the example shown in [Chapter 4 of Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>just plain boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>entirely predictable and lacks energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>no surprises and very few laughs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>very powerful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>the most fun film of the summer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                   text\n",
       "0      0                      just plain boring\n",
       "1      0  entirely predictable and lacks energy\n",
       "2      0       no surprises and very few laughs\n",
       "3      1                          very powerful\n",
       "4      1        the most fun film of the summer"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_data = {'label': [0,0,0,1,1], 'text': ['just plain boring', 'entirely predictable and lacks energy', 'no surprises and very few laughs',\n",
    "'very powerful', 'the most fun film of the summer']}\n",
    "\n",
    "sample_train_data = pd.DataFrame(sample_train_data)\n",
    "sample_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sample_train_data.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for our use\n",
    "def unique_words(corpus):\n",
    "    \"\"\" Given a body of text, returns a set of all unique words contained.\"\"\"\n",
    "    doc = [text.split(' ') for text in corpus]\n",
    "    return set(word for text in doc for word in text)\n",
    "\n",
    "def all_words(corpus):\n",
    "    \"\"\" Given a body of text, returns all words contained\"\"\"\n",
    "    doc = [text.split(' ') for text in corpus]\n",
    "    return [word for text in doc for word in text] \n",
    "\n",
    "def num_class(c, classes=sample_train_data['label']):\n",
    "    \"\"\" Given a class c, returns the number of documents of class c amongst all docs\"\"\"\n",
    "    return (classes == c).sum() \n",
    "\n",
    "def count(w, c):\n",
    "    \"\"\" Returns the occurences of w in bigdoc[c]\"\"\"\n",
    "    i = big_doc[c].count(w)\n",
    "    return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of', 'few', 'the', 'entirely', 'boring', 'lacks', 'surprises', 'energy', 'very', 'plain', 'no', 'powerful', 'laughs', 'just', 'film', 'and', 'most', 'fun', 'predictable', 'summer'}\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary V containing all unique words in training corpus\n",
    "V = unique_words(sample_train_data['text'])\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of Positive words: ['very', 'powerful', 'the', 'most', 'fun', 'film', 'of', 'the', 'summer']\n",
      "Vocabulary of Negative words: ['just', 'plain', 'boring', 'entirely', 'predictable', 'and', 'lacks', 'energy', 'no', 'surprises', 'and', 'very', 'few', 'laughs']\n"
     ]
    }
   ],
   "source": [
    "# To compute P(w|c), we need to find fraction of frequency of word w amongst all words in all documents of class c\n",
    "# For this, we create a big document each, containing all text, corresponding to each class\n",
    "doc_neg = sample_train_data.loc[sample_train_data['label'] == 0, 'text']\n",
    "doc_pos = sample_train_data.loc[sample_train_data['label'] == 1, 'text']\n",
    "\n",
    "doc_pos, doc_neg = all_words(doc_pos), all_words(doc_neg)\n",
    "print(f\"Vocabulary of Positive words: {doc_pos}\")\n",
    "print(f\"Vocabulary of Negative words: {doc_neg}\")\n",
    "\n",
    "# Store in a generic big document:\n",
    "big_doc = {0: doc_neg, 1: doc_pos}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def train_naive_bayes(classes, d=sample_train_data['text']):\n",
    "    \n",
    "    log_prior_dict = dict()\n",
    "    log_likelihood_dict = dict()\n",
    "    for c in classes:\n",
    "        n_doc = len(d) # number of total documents \n",
    "        n_c = num_class(c) # number of documents of class c \n",
    "        log_prior = np.log(n_c/n_doc)\n",
    "        log_prior_dict[c] = log_prior\n",
    "        word_likelihood = dict()\n",
    "        for word in V:\n",
    "            word_count = count(word, c) # occurences of word in bigdoc[c]\n",
    "            log_likelihood = np.log((word_count + 1) / (len(big_doc[c]) + len(V)))\n",
    "            word_likelihood[word] = log_likelihood\n",
    "        log_likelihood_dict[c] = word_likelihood\n",
    "\n",
    "    return log_prior_dict, log_likelihood_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -0.5108256237659907, 1: -0.916290731874155}\n"
     ]
    }
   ],
   "source": [
    "log_prior, log_likelihood = train_naive_bayes(labels)\n",
    "print(log_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(predictable|-) = -2.833213344056216, P(predictable|+) = -3.367295829986474\n",
      "P(no|-) = -2.833213344056216, P(no|+) = -3.367295829986474\n",
      "P(fun|-) = -3.5263605246161616, P(fun|+) = -2.6741486494265287\n"
     ]
    }
   ],
   "source": [
    "# We can now view some sample log probabilities and likelihoods through our implementation:\n",
    "print(f\"P(predictable|-) = {log_likelihood[0]['predictable']}, P(predictable|+) = {log_likelihood[1]['predictable']}\")\n",
    "print(f\"P(no|-) = {log_likelihood[0]['no']}, P(no|+) = {log_likelihood[1]['no']}\")\n",
    "print(f\"P(fun|-) = {log_likelihood[0]['fun']}, P(fun|+) = {log_likelihood[1]['fun']}\")\n",
    "\n",
    "# These values make sense since words that are more frequent in the positive documents, for example, have higher log likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a test example, we can now classify it based on our trained probabilities\n",
    "def test_naive_bayes(test_doc, log_prior, log_likelihood, classes):\n",
    "\n",
    "    \"\"\" Returns best class given a document\"\"\"\n",
    "    class_probs = dict()\n",
    "    for c in classes:\n",
    "        class_probs[c] = log_prior[c]\n",
    "        test_words = all_words(test_doc)\n",
    "        for word in test_words:\n",
    "            if word in V: # ignoring words not encountered in training corpus\n",
    "                class_probs[c] += log_likelihood[c][word]\n",
    "    return max(class_probs, key=class_probs.get) # return class with highest prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of Negative Document: 0\n",
      "Prediction of Positive Document: 1\n"
     ]
    }
   ],
   "source": [
    "test_doc_neg = [\"predictable with no fun\"]\n",
    "test_doc_pos = [\"it was a powerful movie with a lot of fun\"]\n",
    "\n",
    "preds_neg = test_naive_bayes(test_doc_neg, log_prior, log_likelihood, labels) \n",
    "preds_pos = test_naive_bayes(test_doc_pos, log_prior, log_likelihood, labels) \n",
    "print(f\"Prediction of Negative Document: {preds_neg}\")\n",
    "print(f\"Prediction of Positive Document: {preds_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Naive Bayes using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = nb.predict(test_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.992\n",
      "Precision: 1.000\n",
      "Recall: 0.940\n",
      "F1 score: 0.969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, predictions):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, predictions):.3f}\")\n",
    "print(f\"F1 score: {f1_score(y_test, predictions):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c99bd3d91a4b91323b9437e92699dbf82cc6ba26dc500b79c067afe17d1503df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('handson_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
